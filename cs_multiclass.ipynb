{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from ray.train.lightning import (\n",
    "    RayDDPStrategy,\n",
    "    RayLightningEnvironment,\n",
    "    RayTrainReportCallback,\n",
    "    prepare_trainer,\n",
    ")\n",
    "\n",
    "# Set up path to import parent modules\n",
    "from pathlib import Path\n",
    "import sys  \n",
    "\n",
    "# Add to sys.path\n",
    "sys.path.insert(0, str(Path().resolve().parents[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Number of classes 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewbakalar/Documents/Code/cryptic/lit_modules/data_modules.py:54: RuntimeWarning: divide by zero encountered in divide\n",
      "  weight = 1. / class_sample_count\n",
      "\n",
      "  | Name     | Type               | Params\n",
      "------------------------------------------------\n",
      "0 | model    | MLPModel           | 358 K \n",
      "1 | sigmoid  | Sigmoid            | 0     \n",
      "2 | loss_fn  | BCEWithLogitsLoss  | 0     \n",
      "3 | accuracy | MulticlassAccuracy | 0     \n",
      "------------------------------------------------\n",
      "358 K     Trainable params\n",
      "0         Non-trainable params\n",
      "358 K     Total params\n",
      "1.434     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e456faca044815bd538ea0373ee6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([32, 3])) must be the same as input size (torch.Size([32, 2]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/matthewbakalar/Documents/Code/cryptic/cs_multiclass.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/cs_multiclass.ipynb#W3sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m tb_logger \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mloggers\u001b[39m.\u001b[39mTensorBoardLogger(save_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlightning_logs/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/cs_multiclass.ipynb#W3sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, logger\u001b[39m=\u001b[39mtb_logger, default_root_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/cs_multiclass.ipynb#W3sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m trainer\u001b[39m.\u001b[39mfit(lit_model, data_module)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:545\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    544\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m call\u001b[39m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    546\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    547\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:581\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    576\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    577\u001b[0m     ckpt_path,\n\u001b[1;32m    578\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    580\u001b[0m )\n\u001b[0;32m--> 581\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run(model, ckpt_path\u001b[39m=\u001b[39mckpt_path)\n\u001b[1;32m    583\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    584\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    987\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 990\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_stage()\n\u001b[1;32m    992\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    995\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1034\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m   1033\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1034\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1035\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1036\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1063\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1060\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1062\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1063\u001b[0m val_loop\u001b[39m.\u001b[39mrun()\n\u001b[1;32m   1065\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1067\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    180\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[1;32m    133\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[1;32m    135\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    385\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m step_args \u001b[39m=\u001b[39m (\n\u001b[1;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    388\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    389\u001b[0m     \u001b[39melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    390\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39mstep_args)\n\u001b[1;32m    393\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    396\u001b[0m     \u001b[39m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[1;32m    402\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 403\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mvalidation_step(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/Code/cryptic/lit_modules/modules.py:57\u001b[0m, in \u001b[0;36mClassifier.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     55\u001b[0m data, target \u001b[39m=\u001b[39m batch\n\u001b[1;32m     56\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(data)\n\u001b[0;32m---> 57\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(logits, target\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m     58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogging(logits, target, loss, \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:725\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 725\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target,\n\u001b[1;32m    726\u001b[0m                                               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight,\n\u001b[1;32m    727\u001b[0m                                               pos_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_weight,\n\u001b[1;32m    728\u001b[0m                                               reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduction)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3193\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3190\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[0;32m-> 3193\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3195\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([32, 3])) must be the same as input size (torch.Size([32, 2]))"
     ]
    }
   ],
   "source": [
    "import models as models\n",
    "from lit_modules import data_modules, modules\n",
    "\n",
    "data_path = 'data/TB000208a'\n",
    "genomic_reference_file = '../data/reference/hg38.fa'\n",
    "n_classes = 3\n",
    "seq_length = 46\n",
    "vocab_size = 4\n",
    "input_size = seq_length*vocab_size\n",
    "hidden_size = 512\n",
    "n_hidden = 2\n",
    "train_test_split = 0.8\n",
    "\n",
    "# Build the data module\n",
    "data_module = data_modules.MulticlassDataModule(\n",
    "    data_path, \n",
    "    threshold=0.01, \n",
    "    genomic_reference_file=genomic_reference_file,\n",
    "    n_classes=n_classes, \n",
    "    train_test_split=train_test_split, \n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Build model\n",
    "lit_model = modules.Classifier(input_size, hidden_size, n_classes, n_hidden, dropout=0.5)\n",
    "\n",
    "# train the model\n",
    "tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"lightning_logs/\")\n",
    "trainer = pl.Trainer(max_epochs=5, logger=tb_logger, default_root_dir='.')\n",
    "trainer.fit(lit_model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (model): MLPModel(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear_relu_stack): Sequential(\n",
       "      (0): Linear(in_features=184, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       "  (loss_fn): BCEWithLogitsLoss()\n",
       "  (accuracy): MulticlassAccuracy()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-11-23 08:09:41</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:38.24        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.8/16.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Logical resource usage: 2.0/8 CPUs, 1.0/1 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 3<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_79508_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/matthewbakalar/ray_results/TorchTrainer_2023-11-23_08-09-02/TorchTrainer_79508_00000_0_batch_size=32,hidden_size=512,lr=0.0022_2023-11-23_08-09-03/error.txt</td></tr>\n",
       "<tr><td>TorchTrainer_79508_00001</td><td style=\"text-align: right;\">           1</td><td>/Users/matthewbakalar/ray_results/TorchTrainer_2023-11-23_08-09-02/TorchTrainer_79508_00001_1_batch_size=64,hidden_size=512,lr=0.0843_2023-11-23_08-09-03/error.txt</td></tr>\n",
       "<tr><td>TorchTrainer_79508_00002</td><td style=\"text-align: right;\">           1</td><td>/Users/matthewbakalar/ray_results/TorchTrainer_2023-11-23_08-09-02/TorchTrainer_79508_00002_2_batch_size=32,hidden_size=512,lr=0.0600_2023-11-23_08-09-03/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">   train_loop_config/ba\n",
       "tch_size</th><th style=\"text-align: right;\">     train_loop_config/hi\n",
       "dden_size</th><th style=\"text-align: right;\">  train_loop_config/lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_79508_00003</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">           0.0025049  </td></tr>\n",
       "<tr><td>TorchTrainer_79508_00004</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">           0.0022036  </td></tr>\n",
       "<tr><td>TorchTrainer_79508_00005</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">64</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">           0.0547193  </td></tr>\n",
       "<tr><td>TorchTrainer_79508_00006</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">           0.00266909 </td></tr>\n",
       "<tr><td>TorchTrainer_79508_00007</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">           0.0472886  </td></tr>\n",
       "<tr><td>TorchTrainer_79508_00008</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">           0.0389879  </td></tr>\n",
       "<tr><td>TorchTrainer_79508_00009</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">           0.000128937</td></tr>\n",
       "<tr><td>TorchTrainer_79508_00000</td><td>ERROR   </td><td>127.0.0.1:27970</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">           0.00215776 </td></tr>\n",
       "<tr><td>TorchTrainer_79508_00001</td><td>ERROR   </td><td>127.0.0.1:27977</td><td style=\"text-align: right;\">64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">           0.0842684  </td></tr>\n",
       "<tr><td>TorchTrainer_79508_00002</td><td>ERROR   </td><td>127.0.0.1:27985</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">           0.0600417  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=27970)\u001b[0m Starting distributed worker processes: ['27973 (127.0.0.1)']\n",
      "\u001b[36m(RayTrainWorker pid=27973)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "2023-11-23 08:09:14,065\tERROR tune_controller.py:1383 -- Trial task failed for trial TorchTrainer_79508_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py\", line 2563, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=27970, ip=127.0.0.1, actor_id=96609a9cbcff7ae63843e50901000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/air/_internal/util.py\", line 91, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/base_trainer.py\", line 797, in _trainable_func\n",
      "    super()._trainable_func(self._merged_config)\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "    output = fn()\n",
      "             ^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/base_trainer.py\", line 707, in train_func\n",
      "    trainer.training_loop()\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/data_parallel_trainer.py\", line 448, in training_loop\n",
      "    backend_executor.start()\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/_internal/backend_executor.py\", line 196, in start\n",
      "    self._backend.on_start(self.worker_group, self._backend_config)\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/torch/config.py\", line 198, in on_start\n",
      "    ray.get(setup_futures)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute._setup_torch_process_group()\u001b[39m (pid=27973, ip=127.0.0.1, actor_id=889808501f75e2bbfab2550501000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x123bf20d0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 30, in __execute\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/torch/config.py\", line 106, in _setup_torch_process_group\n",
      "    dist.init_process_group(\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/torch/distributed/c10d_logger.py\", line 74, in wrapper\n",
      "    func_return = func(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\", line 1148, in init_process_group\n",
      "    default_pg, _ = _new_process_group_helper(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\", line 1268, in _new_process_group_helper\n",
      "    raise RuntimeError(\"Distributed package doesn't have NCCL built in\")\n",
      "RuntimeError: Distributed package doesn't have NCCL built in\n",
      "\u001b[36m(TorchTrainer pid=27977)\u001b[0m Starting distributed worker processes: ['27983 (127.0.0.1)']\n",
      "2023-11-23 08:09:24,485\tERROR tune_controller.py:1383 -- Trial task failed for trial TorchTrainer_79508_00001\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py\", line 2563, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=27977, ip=127.0.0.1, actor_id=da76679211e948e2711056b501000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/air/_internal/util.py\", line 91, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/base_trainer.py\", line 797, in _trainable_func\n",
      "    super()._trainable_func(self._merged_config)\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "    output = fn()\n",
      "             ^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/base_trainer.py\", line 707, in train_func\n",
      "    trainer.training_loop()\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/data_parallel_trainer.py\", line 448, in training_loop\n",
      "    backend_executor.start()\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/_internal/backend_executor.py\", line 196, in start\n",
      "    self._backend.on_start(self.worker_group, self._backend_config)\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/torch/config.py\", line 198, in on_start\n",
      "    ray.get(setup_futures)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute._setup_torch_process_group()\u001b[39m (pid=27983, ip=127.0.0.1, actor_id=5963f65956631aae4a05e55301000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x10fef8410>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 30, in __execute\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/torch/config.py\", line 106, in _setup_torch_process_group\n",
      "    dist.init_process_group(\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/torch/distributed/c10d_logger.py\", line 74, in wrapper\n",
      "    func_return = func(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\", line 1148, in init_process_group\n",
      "    default_pg, _ = _new_process_group_helper(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\", line 1268, in _new_process_group_helper\n",
      "    raise RuntimeError(\"Distributed package doesn't have NCCL built in\")\n",
      "RuntimeError: Distributed package doesn't have NCCL built in\n",
      "\u001b[36m(RayTrainWorker pid=27983)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=27985)\u001b[0m Starting distributed worker processes: ['27986 (127.0.0.1)']\n",
      "\u001b[36m(RayTrainWorker pid=27986)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "2023-11-23 08:09:35,576\tERROR tune_controller.py:1383 -- Trial task failed for trial TorchTrainer_79508_00002\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py\", line 2563, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=27985, ip=127.0.0.1, actor_id=2f53579f88c0ddde696bfda301000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/air/_internal/util.py\", line 91, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/base_trainer.py\", line 797, in _trainable_func\n",
      "    super()._trainable_func(self._merged_config)\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "    output = fn()\n",
      "             ^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/base_trainer.py\", line 707, in train_func\n",
      "    trainer.training_loop()\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/data_parallel_trainer.py\", line 448, in training_loop\n",
      "    backend_executor.start()\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/_internal/backend_executor.py\", line 196, in start\n",
      "    self._backend.on_start(self.worker_group, self._backend_config)\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/torch/config.py\", line 198, in on_start\n",
      "    ray.get(setup_futures)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute._setup_torch_process_group()\u001b[39m (pid=27986, ip=127.0.0.1, actor_id=8b6dfc2d78d19ec8cb7636a901000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x126415e90>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 30, in __execute\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/ray/train/torch/config.py\", line 106, in _setup_torch_process_group\n",
      "    dist.init_process_group(\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/torch/distributed/c10d_logger.py\", line 74, in wrapper\n",
      "    func_return = func(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\", line 1148, in init_process_group\n",
      "    default_pg, _ = _new_process_group_helper(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\", line 1268, in _new_process_group_helper\n",
      "    raise RuntimeError(\"Distributed package doesn't have NCCL built in\")\n",
      "RuntimeError: Distributed package doesn't have NCCL built in\n",
      "2023-11-23 08:09:41,265\tWARNING tune.py:186 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2023-11-23 08:09:42,936\tERROR tune.py:1043 -- Trials did not complete: [TorchTrainer_79508_00000, TorchTrainer_79508_00001, TorchTrainer_79508_00002]\n",
      "2023-11-23 08:09:42,936\tINFO tune.py:1047 -- Total run time: 39.94 seconds (38.23 seconds for the tuning loop).\n",
      "2023-11-23 08:09:42,937\tWARNING tune.py:1062 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"/Users/matthewbakalar/ray_results/TorchTrainer_2023-11-23_08-09-02\", trainable=...)\n",
      "2023-11-23 08:09:42,948\tWARNING experiment_analysis.py:185 -- Failed to fetch metrics for 7 trial(s):\n",
      "- TorchTrainer_79508_00003: FileNotFoundError('Could not fetch metrics for TorchTrainer_79508_00003: both result.json and progress.csv were not found at /Users/matthewbakalar/ray_results/TorchTrainer_2023-11-23_08-09-02/TorchTrainer_79508_00003_3_batch_size=64,hidden_size=2048,lr=0.0025_2023-11-23_08-09-03')\n",
      "- TorchTrainer_79508_00004: FileNotFoundError('Could not fetch metrics for TorchTrainer_79508_00004: both result.json and progress.csv were not found at /Users/matthewbakalar/ray_results/TorchTrainer_2023-11-23_08-09-02/TorchTrainer_79508_00004_4_batch_size=32,hidden_size=2048,lr=0.0022_2023-11-23_08-09-03')\n",
      "- TorchTrainer_79508_00005: FileNotFoundError('Could not fetch metrics for TorchTrainer_79508_00005: both result.json and progress.csv were not found at /Users/matthewbakalar/ray_results/TorchTrainer_2023-11-23_08-09-02/TorchTrainer_79508_00005_5_batch_size=64,hidden_size=2048,lr=0.0547_2023-11-23_08-09-03')\n",
      "- TorchTrainer_79508_00006: FileNotFoundError('Could not fetch metrics for TorchTrainer_79508_00006: both result.json and progress.csv were not found at /Users/matthewbakalar/ray_results/TorchTrainer_2023-11-23_08-09-02/TorchTrainer_79508_00006_6_batch_size=32,hidden_size=512,lr=0.0027_2023-11-23_08-09-03')\n",
      "- TorchTrainer_79508_00007: FileNotFoundError('Could not fetch metrics for TorchTrainer_79508_00007: both result.json and progress.csv were not found at /Users/matthewbakalar/ray_results/TorchTrainer_2023-11-23_08-09-02/TorchTrainer_79508_00007_7_batch_size=32,hidden_size=512,lr=0.0473_2023-11-23_08-09-03')\n",
      "- TorchTrainer_79508_00008: FileNotFoundError('Could not fetch metrics for TorchTrainer_79508_00008: both result.json and progress.csv were not found at /Users/matthewbakalar/ray_results/TorchTrainer_2023-11-23_08-09-02/TorchTrainer_79508_00008_8_batch_size=32,hidden_size=1024,lr=0.0390_2023-11-23_08-09-03')\n",
      "- TorchTrainer_79508_00009: FileNotFoundError('Could not fetch metrics for TorchTrainer_79508_00009: both result.json and progress.csv were not found at /Users/matthewbakalar/ray_results/TorchTrainer_2023-11-23_08-09-02/TorchTrainer_79508_00009_9_batch_size=64,hidden_size=1024,lr=0.0001_2023-11-23_08-09-03')\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "# Init the cluster for training\n",
    "ray.shutdown()\n",
    "ray.init(num_cpus=8, num_gpus=1)\n",
    "ray.init()\n",
    "\n",
    "# Model tuning\n",
    "def train_func():\n",
    "    # Build the lightning modules\n",
    "    data_module = data_modules.MulticlassDataModule(data_path, threshold=0.01, n_classes=n_classes, train_test_split=train_test_split, batch_size=32)\n",
    "    lit_model = modules.Classifier(input_size=input_size, hidden_size=hidden_size, output_size=n_classes, n_hidden=n_hidden, dropout=0.5)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        devices=\"auto\",\n",
    "        accelerator=\"auto\",\n",
    "        strategy=RayDDPStrategy(),\n",
    "        callbacks=[RayTrainReportCallback()],\n",
    "        plugins=[RayLightningEnvironment()],\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "    trainer = prepare_trainer(trainer)\n",
    "    trainer.fit(lit_model, datamodule=data_module)\n",
    "\n",
    "search_space = {\n",
    "    \"hidden_size\": tune.choice([512, 1024, 2048]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"batch_size\": tune.choice([32, 64]),\n",
    "}\n",
    "\n",
    "# The maximum training epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Number of sampls from parameter space\n",
    "num_samples = 10\n",
    "\n",
    "scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n",
    "\n",
    "from ray.train import RunConfig, ScalingConfig, CheckpointConfig\n",
    "\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=1, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        num_to_keep=2,\n",
    "        checkpoint_score_attribute=\"val_acc\",\n",
    "        checkpoint_score_order=\"max\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "# Define a TorchTrainer without hyper-parameters for Tuner\n",
    "ray_trainer = TorchTrainer(\n",
    "    train_func,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")\n",
    "\n",
    "def tune_mnist_asha(num_samples=10):\n",
    "    scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        ray_trainer,\n",
    "        param_space={\"train_loop_config\": search_space},\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"val_acc\",\n",
    "            mode=\"max\",\n",
    "            num_samples=num_samples,\n",
    "            scheduler=scheduler,\n",
    "        ),\n",
    "    )\n",
    "    return tuner.fit()\n",
    "\n",
    "\n",
    "results = tune_mnist_asha(num_samples=num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03459d3af554db2aee7ba5bf11b9415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_acc_step         0.7878776788711548\n",
      "        test_loss            0.526176929473877\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc_step': 0.7878776788711548, 'test_loss': 0.526176929473877}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(lit_model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f79316b7b5f4acd828e17abb0127e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewbakalar/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb Cell 7\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m pred_data_module \u001b[39m=\u001b[39m data_modules\u001b[39m.\u001b[39mGenomeDataModule(genomic_reference_file, batch_size\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m preds \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mpredict(lit_model, pred_data_module)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mhstack(preds[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Fast prediction code. Currently runs on one chromosome only\n",
    "genomic_reference_file = '../../data/reference/hg38.fa'\n",
    "\n",
    "pred_data_module = data_modules.GenomeDataModule(genomic_reference_file, batch_size=256)\n",
    "preds = trainer.predict(lit_model, pred_data_module)\n",
    "preds = torch.hstack(preds[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chr1...\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "1260000\n",
      "1270000\n",
      "1280000\n",
      "1290000\n",
      "1300000\n",
      "1310000\n",
      "1320000\n",
      "1330000\n",
      "1340000\n",
      "1350000\n",
      "1360000\n",
      "1370000\n",
      "1380000\n",
      "1390000\n",
      "1400000\n",
      "1410000\n",
      "1420000\n",
      "1430000\n",
      "1440000\n",
      "1450000\n",
      "1460000\n",
      "1470000\n",
      "1480000\n",
      "1490000\n",
      "1500000\n",
      "1510000\n",
      "1520000\n",
      "1530000\n",
      "1540000\n",
      "1550000\n",
      "1560000\n",
      "1570000\n",
      "1580000\n",
      "1590000\n",
      "1600000\n",
      "1610000\n",
      "1620000\n",
      "1630000\n",
      "1640000\n",
      "1650000\n",
      "1660000\n",
      "1670000\n",
      "1680000\n",
      "1690000\n",
      "1700000\n",
      "1710000\n",
      "1720000\n",
      "1730000\n",
      "1740000\n",
      "1750000\n",
      "1760000\n",
      "1770000\n",
      "1780000\n",
      "1790000\n",
      "1800000\n",
      "1810000\n",
      "1820000\n",
      "1830000\n",
      "1840000\n",
      "1850000\n",
      "1860000\n",
      "1870000\n",
      "1880000\n",
      "1890000\n",
      "1900000\n",
      "1910000\n",
      "1920000\n",
      "1930000\n",
      "1940000\n",
      "1950000\n",
      "1960000\n",
      "1970000\n",
      "1980000\n",
      "1990000\n",
      "2000000\n",
      "2010000\n",
      "2020000\n",
      "2030000\n",
      "2040000\n",
      "2050000\n",
      "2060000\n",
      "2070000\n",
      "2080000\n",
      "2090000\n",
      "2100000\n",
      "2110000\n",
      "2120000\n",
      "2130000\n",
      "2140000\n",
      "2150000\n",
      "2160000\n",
      "2170000\n",
      "2180000\n",
      "2190000\n",
      "2200000\n",
      "2210000\n",
      "2220000\n",
      "2230000\n",
      "2240000\n",
      "2250000\n",
      "2260000\n",
      "2270000\n",
      "2280000\n",
      "2290000\n",
      "2300000\n",
      "2310000\n",
      "2320000\n",
      "2330000\n",
      "2340000\n",
      "2350000\n",
      "2360000\n",
      "2370000\n",
      "2380000\n",
      "2390000\n",
      "2400000\n",
      "2410000\n",
      "2420000\n",
      "2430000\n",
      "2440000\n",
      "2450000\n",
      "2460000\n",
      "2470000\n",
      "2480000\n",
      "2490000\n",
      "2500000\n",
      "2510000\n",
      "2520000\n",
      "2530000\n",
      "2540000\n",
      "2550000\n",
      "2560000\n",
      "2570000\n",
      "2580000\n",
      "2590000\n",
      "2600000\n",
      "2610000\n",
      "2620000\n",
      "2630000\n",
      "2640000\n",
      "2650000\n",
      "2660000\n",
      "2670000\n",
      "2680000\n",
      "2690000\n",
      "2700000\n",
      "2710000\n",
      "2720000\n",
      "2730000\n",
      "2740000\n",
      "2750000\n",
      "2760000\n",
      "2770000\n",
      "2780000\n",
      "2790000\n",
      "2800000\n",
      "2810000\n",
      "2820000\n",
      "2830000\n",
      "2840000\n",
      "2850000\n",
      "2860000\n",
      "2870000\n",
      "2880000\n",
      "2890000\n",
      "2900000\n",
      "2910000\n",
      "2920000\n",
      "2930000\n",
      "2940000\n",
      "2950000\n",
      "2960000\n",
      "2970000\n",
      "2980000\n",
      "2990000\n",
      "3000000\n",
      "3010000\n",
      "3020000\n",
      "3030000\n",
      "3040000\n",
      "3050000\n",
      "3060000\n",
      "3070000\n",
      "3080000\n",
      "3090000\n",
      "3100000\n",
      "3110000\n",
      "3120000\n",
      "3130000\n",
      "3140000\n",
      "3150000\n",
      "3160000\n",
      "3170000\n",
      "3180000\n",
      "3190000\n",
      "3200000\n",
      "3210000\n",
      "3220000\n",
      "3230000\n",
      "3240000\n",
      "3250000\n",
      "3260000\n",
      "3270000\n",
      "3280000\n",
      "3290000\n",
      "3300000\n",
      "3310000\n",
      "3320000\n",
      "3330000\n",
      "3340000\n",
      "3350000\n",
      "3360000\n",
      "3370000\n",
      "3380000\n",
      "3390000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb Cell 12\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb#X12sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m chromosome_id \u001b[39m=\u001b[39m record\u001b[39m.\u001b[39mid\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb#X12sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProcessing \u001b[39m\u001b[39m{\u001b[39;00mchromosome_id\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb#X12sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m predictions \u001b[39m=\u001b[39m sliding_window_inference(\u001b[39mstr\u001b[39m(chromosome_sequence), seq_length, batch_size)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb#X12sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39mprint\u001b[39m(predictions)\n",
      "\u001b[1;32m/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb#X12sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m back_half_sequence \u001b[39m=\u001b[39m reverse_complement(full_sequence[\u001b[39m24\u001b[39m:])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb#X12sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m encoded_seqs_front\u001b[39m.\u001b[39mappend(encode_sequence(front_half_sequence, seq_length))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb#X12sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m encoded_seqs_back\u001b[39m.\u001b[39mappend(encode_sequence(back_half_sequence, seq_length))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb#X12sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoded_seqs_front) \u001b[39m==\u001b[39m batch_size:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb#X12sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39m# Make predictions on batch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb#X12sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     batch_preds \u001b[39m=\u001b[39m predict_on_batch(encoded_seqs_front, encoded_seqs_back)\n",
      "\u001b[1;32m/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m translation_dict \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m0\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mT\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m1\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m2\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mG\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m3\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mN\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m4\u001b[39m}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m encoding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([translation_dict[c] \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m seq])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mone_hot(encoding, num_classes\u001b[39m=\u001b[39mvocab_size)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matthewbakalar/Documents/Code/cryptic/Notebooks/cs_multiclass.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "\n",
    "lit_model.eval()\n",
    "\n",
    "from Bio import SeqIO  #\n",
    "import torch.nn.functional as F\n",
    "\n",
    "genomic_reference_file = '../../data/reference/hg38.fa'\n",
    "\n",
    "\n",
    "def reverse_complement(dna_sequence):\n",
    "    complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n",
    "    reversed_sequence = dna_sequence[::-1]\n",
    "    reverse_complement_sequence = ''.join(complement[nucleotide] for nucleotide in reversed_sequence)\n",
    "    return reverse_complement_sequence\n",
    "\n",
    "def encode_sequence(seq, seq_length=46, vocab_size=5):\n",
    "    translation_dict = {'A':0,'T':1,'C':2,'G':3,'N':4}\n",
    "    encoding = torch.tensor([translation_dict[c] for c in seq])\n",
    "    x = F.one_hot(encoding, num_classes=vocab_size).to(torch.float32)\n",
    "    return x\n",
    "\n",
    "# Adjust the sliding window function to use batches\n",
    "def sliding_window_inference(genome_sequence, seq_length, batch_size):\n",
    "    predictions = []\n",
    "    encoded_seqs_front = []\n",
    "    encoded_seqs_back = []\n",
    "    \n",
    "    for i in range(0, len(genome_sequence) - seq_length + 1):\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        # Check for 'N' early\n",
    "        full_sequence = genome_sequence[i:i+seq_length]\n",
    "        if 'N' in full_sequence:\n",
    "            continue\n",
    "        \n",
    "        # Process in batches\n",
    "        front_half_sequence = full_sequence[:22]\n",
    "        back_half_sequence = reverse_complement(full_sequence[24:])\n",
    "        \n",
    "        encoded_seqs_front.append(encode_sequence(front_half_sequence, seq_length))\n",
    "        encoded_seqs_back.append(encode_sequence(back_half_sequence, seq_length))\n",
    "        \n",
    "        if len(encoded_seqs_front) == batch_size:\n",
    "            # Make predictions on batch\n",
    "            batch_preds = predict_on_batch(encoded_seqs_front, encoded_seqs_back)\n",
    "            predictions.extend(batch_preds)\n",
    "            \n",
    "            # Clear lists for next batch\n",
    "            encoded_seqs_front = []\n",
    "            encoded_seqs_back = []\n",
    "\n",
    "    # Process the final batch if there are any sequences left\n",
    "    if encoded_seqs_front:\n",
    "        batch_preds = predict_on_batch(encoded_seqs_front, encoded_seqs_back)\n",
    "        predictions.extend(batch_preds)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Define a function to make predictions on batches\n",
    "def predict_on_batch(front_seqs, back_seqs):\n",
    "    front_seqs_tensor = torch.stack(front_seqs)\n",
    "    back_seqs_tensor = torch.stack(back_seqs)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        front_preds = lit_model.predict_step((front_seqs_tensor, None), 0)\n",
    "        back_preds = lit_model.predict_step((back_seqs_tensor, None), 0)\n",
    "        average_logits = (front_preds + back_preds) / 2\n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "        final_preds = sigmoid(average_logits)\n",
    "    \n",
    "    # print(final_preds)\n",
    "        \n",
    "    return final_preds.tolist()\n",
    "\n",
    "\n",
    "# Process each sequence in the FASTA file\n",
    "seq_length = 46\n",
    "batch_size = 10000  # or any size that fits in your GPU memory\n",
    "for record in SeqIO.parse(genomic_reference_file, \"fasta\"):\n",
    "    chromosome_sequence = record.seq.upper()\n",
    "    chromosome_id = record.id\n",
    "    print(f\"Processing {chromosome_id}...\")\n",
    "    \n",
    "    predictions = sliding_window_inference(str(chromosome_sequence), seq_length, batch_size)\n",
    "    print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix this to unzip a tuple\n",
    "data = list(data_module.predict_dataloader())\n",
    "inputs, labels = map(list, zip(*data))\n",
    "inputs = torch.vstack(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
